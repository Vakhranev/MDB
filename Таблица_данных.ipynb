{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlBi6vMqARAccE8DFfJOgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vakhranev/MDB/blob/main/%D0%A2%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D0%B0_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import csv\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLQgb0uquhmN",
        "outputId": "3c1d2782-1c11-4c07-b04a-5101d688fc61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# распаковываем архив\n",
        "fantasy_zip = zipfile.ZipFile('Z from Sasha Transcribed data txt Summer STANDARDIZED 2017 2018 2019 2020.zip')\n",
        "fantasy_zip.extractall()\n",
        "fantasy_zip.close()"
      ],
      "metadata": {
        "id": "66YQ98iRTl_j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path =\"Z from Sasha Transcribed data txt Summer STANDARDIZED 2017 2018 2019 2020\"\n",
        "filelist = []\n",
        "\n",
        "for root, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "    if str(file)[-3:]=='txt':\n",
        "      filelist.append(os.path.join(root,file))\n",
        "\n",
        "print(len(filelist))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBaVV3h8T_a6",
        "outputId": "18ec7d01-6d2d-4463-f420-aaa4912413a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# создаём csv-файл и пока вносим в него только названия колонок\n",
        "with open('mdb_files.csv', 'w') as csvfile:\n",
        "  filewriter = csv.writer(csvfile, delimiter=';',\n",
        "  quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "  filewriter.writerow(['File Name', 'Length', 'First Name', 'Last Name', 'Entry/Exit', 'Mark'])\n",
        "for name in filelist:\n",
        "  # вытаскиваем названия файлов из архива\n",
        "  FileName = re.split(r'/', name)[-1]\n",
        "  Names_of_file=re.split(r'\\d', FileName)[0]\n",
        "  # разбиваем названия файла по нижним подчёркиваниям\n",
        "  List_with_names = Names_of_file.split(\"_\")\n",
        "  # делаем строчный регистр и группируем имена правильно, если имён двое, например\n",
        "  FN = List_with_names[0].lower()\n",
        "  if len(List_with_names) > 3:\n",
        "    List_with_names[0] = List_with_names[0] + \" \" + List_with_names[1]\n",
        "    List_with_names[1] = List_with_names[2]\n",
        "    List_with_names[2] = List_with_names[3]\n",
        "  # print(List_with_names)\n",
        "  List_with_names[0] = List_with_names[0].lower()\n",
        "  List_with_names[1] = List_with_names[1].lower()\n",
        "  df = pd.read_csv('metadata.csv', sep=',')\n",
        "  df.index = np.arange(1, len(df) + 1)\n",
        "  # в именах из файла тоже делаем строчный регистр\n",
        "  # (потому что в названиях файла и таблице не везде заглавные буквы совпадали)\n",
        "  df2 = df['Last_Name'].str.lower()\n",
        "  df['Last_Name'] = df2\n",
        "  df3 = df['First_Name'].str.lower()\n",
        "  df['First_Name'] = df3\n",
        "  # проверяем, совпадают ли имена в названиях файла с именами таблицы\n",
        "  df1 = df[df[\"First_Name\"].str.contains(FN)]\n",
        "  df1 = df1[df1[\"Last_Name\"].str.contains(List_with_names[1])]\n",
        "  if df1.empty:\n",
        "    df1 = df[df[\"Last_Name\"].str.contains(FN)]\n",
        "    df1 = df1[df1[\"First_Name\"].str.contains(List_with_names[1])]\n",
        "  if df1.empty:\n",
        "    df1 = df.query(\"First_Name == @List_with_names[0]\")\n",
        "    df1 = df1.query(\"Last_Name == @List_with_names[1]\")\n",
        "  if df1.empty:\n",
        "    df1 = df.query(\"Last_Name == @List_with_names[0]\")\n",
        "    df1 = df1.query(\"First_Name == @List_with_names[1]\")\n",
        "  # проверяем не содержат ли имена в таблице нижние подчёркивания вместто каких-нибудь символов\n",
        "  if df1.empty:\n",
        "    df1 = df[df[\"Last_Name\"].str.contains('_')]\n",
        "    df1 = df1[df1[\"First_Name\"].str.contains(List_with_names[0])]\n",
        "  # вытаскиваем оценки\n",
        "  if List_with_names[2] == 'Entry':\n",
        "    mark = str(df1['Entry'])\n",
        "    mark=re.split(r'\\s', mark)[-5]\n",
        "  else:\n",
        "    mark = str(df1['Exit'])\n",
        "    mark=re.split(r'\\s', mark)[-5]\n",
        "  # загоняем всё в новый файл\n",
        "  with open(name, 'r') as file:\n",
        "      lines_in_file = file.read()\n",
        "    \n",
        "      nltk_tokens = nltk.word_tokenize(lines_in_file)\n",
        "      nltk_tokens = \" \".join(word for word in nltk_tokens if word not in ('!','.',':',',','–','?','(',')',';','`','<','>'))\n",
        "      nltk_tokens = nltk.word_tokenize(nltk_tokens)\n",
        "      with open('mdb_files.csv', 'a') as csvfile:\n",
        "          filewriter = csv.writer(csvfile, delimiter=';',\n",
        "          quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "          filewriter.writerow([FileName, len(nltk_tokens), List_with_names[0], List_with_names[1], List_with_names[2], mark])"
      ],
      "metadata": {
        "id": "7AqLQDDnW4Bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74329262-dd33-4831-e6c0-36c795325a3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-27b834a377f3>:28: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  df1 = df[df[\"First_Name\"].str.contains(FN)]\n",
            "<ipython-input-11-27b834a377f3>:33: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  df1 = df[df[\"Last_Name\"].str.contains(FN)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sz8RBvjpYUFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}